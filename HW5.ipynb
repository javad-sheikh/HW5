{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('vid1.mp4')\n",
    "template = cv2.imread(\"template.jpg\")\n",
    "GOOD_MATCH_PERCENT = 0.90\n",
    "counter = 0\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    counter+=1\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(counter%10==0):\n",
    "        if(ret==False):\n",
    "            break\n",
    "\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         retval,thresholded = cv2.threshold(gray,thresh=70,maxval=250,type=cv2.THRESH_BINARY)\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=5, nOctaveLayers=3,\n",
    "                                            contrastThreshold = 0.10, edgeThreshold=10,\n",
    "                                            sigma=2)\n",
    "        keypoints1, descriptors1 = sift.detectAndCompute(frame,None)\n",
    "        keypoints2, descriptors2 = sift.detectAndCompute(template,None)\n",
    "        \n",
    "         # Match features.\n",
    "        matcher = cv2.BFMatcher_create(normType = cv2.NORM_L1,crossCheck = False)\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        \n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "#         Find homography\n",
    "        H,mask = cv2.findHomography(points1, points2, cv2.LMEDS,5)\n",
    "        \n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w,_ = template.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "#         \n",
    "        pts =[pts[0][0],pts[1][0],pts[2][0],pts[3][0]]\n",
    "        pts = np.asarray(pts)\n",
    "        pts = np.array([pts])\n",
    "        \n",
    "        try:\n",
    "            dst = cv2.perspectiveTransform(pts,H)\n",
    "            frame = cv2.polylines(frame,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        except:\n",
    "            print(\"error perspectiveTransform\")\n",
    "        \n",
    "        # Draw top matches\n",
    "        img_matches = cv2.drawMatches(frame, keypoints1, template, keypoints2, matches, None)\n",
    "\n",
    "        cv2.imshow('video', img_matches)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('vid1.mp4')\n",
    "template = cv2.imread(\"template.jpg\")\n",
    "GOOD_MATCH_PERCENT = 0.01\n",
    "counter = 0\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    counter+=1\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(counter%10==0):\n",
    "        if(ret==False):\n",
    "            break\n",
    "        surf = cv2.xfeatures2d.SURF_create(hessianThreshold = 2200,nOctaves =8,nOctaveLayers = 2)\n",
    "        keypoints1, descriptors1 = surf.detectAndCompute(frame,None)\n",
    "        keypoints2, descriptors2 = surf.detectAndCompute(template,None)\n",
    "        \n",
    "         # Match features.\n",
    "        matcher = cv2.BFMatcher_create(normType = cv2.NORM_L1,crossCheck = False)\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        \n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "       #         Find homography\n",
    "        H,mask = cv2.findHomography(points1, points2, cv2.RANSAC,5)\n",
    "        \n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w,_ = template.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "#         \n",
    "        pts =[pts[0][0],pts[1][0],pts[2][0],pts[3][0]]\n",
    "        pts = np.asarray(pts)\n",
    "        pts = np.array([pts])\n",
    "        \n",
    "        try:\n",
    "            dst = cv2.perspectiveTransform(pts,H)\n",
    "            frame = cv2.polylines(frame,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        except:\n",
    "            print(\"error perspectiveTransform\")\n",
    "        \n",
    "        # Draw top matches\n",
    "        img_matches = cv2.drawMatches(frame, keypoints1, template, keypoints2, matches, None)\n",
    "\n",
    "        cv2.imshow('video', img_matches)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n",
      "error perspectiveTransform\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('vid1.mp4')\n",
    "template = cv2.imread(\"template.jpg\")\n",
    "GOOD_MATCH_PERCENT = 0.8\n",
    "counter = 0\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    counter+=1\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(counter%10==0):\n",
    "        if(ret==False):\n",
    "            break\n",
    "        freak = cv2.xfeatures2d.FREAK_create()\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=7, nOctaveLayers=3,\n",
    "                                    contrastThreshold = 0.10, edgeThreshold=10,\n",
    "                                    sigma=2)\n",
    "        keypoints1, descriptors1 = sift.detectAndCompute(frame,None)\n",
    "        keypoints2, descriptors2 = sift.detectAndCompute(template,None)\n",
    "        keypoints1, descriptors1 = freak.compute(frame,keypoints1)\n",
    "        keypoints2, descriptors2 = freak.compute(template,keypoints2)\n",
    "        \n",
    "         # Match features.\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=False)\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "           #         Find homography\n",
    "        H,mask = cv2.findHomography(points1, points2, cv2.LMEDS,5)\n",
    "        \n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w,_ = template.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "#         \n",
    "        pts =[pts[0][0],pts[1][0],pts[2][0],pts[3][0]]\n",
    "        pts = np.asarray(pts)\n",
    "        pts = np.array([pts])\n",
    "        \n",
    "        try:\n",
    "            dst = cv2.perspectiveTransform(pts,H)\n",
    "            frame = cv2.polylines(frame,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        except:\n",
    "            print(\"error perspectiveTransform\")\n",
    "        \n",
    "        # Draw top matches\n",
    "        img_matches = cv2.drawMatches(frame, keypoints1, template, keypoints2, matches, None)\n",
    "\n",
    "        cv2.imshow('video', img_matches)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('vid1.mp4')\n",
    "template = cv2.imread(\"template.jpg\")\n",
    "GOOD_MATCH_PERCENT = 0.8\n",
    "counter = 0\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    counter+=1\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(counter%10==0):\n",
    "        if(ret==False):\n",
    "            break\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=100, nOctaveLayers=3,\n",
    "                                    contrastThreshold = 0.10, edgeThreshold=10,\n",
    "                                    sigma=2)\n",
    "        keypoints1, descriptors1 = sift.detectAndCompute(frame,None)\n",
    "        keypoints2, descriptors2 = sift.detectAndCompute(template,None)\n",
    "        brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "        keypoints1, descriptors1 = brief.compute(frame,keypoints1)\n",
    "        keypoints2, descriptors2 = brief.compute(template,keypoints2)\n",
    "        \n",
    "                # Match features.\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=False)\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "           #         Find homography\n",
    "        H,mask = cv2.findHomography(points1, points2, cv2.RANSAC,5)\n",
    "        \n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w,_ = template.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "#         \n",
    "        pts =[pts[0][0],pts[1][0],pts[2][0],pts[3][0]]\n",
    "        pts = np.asarray(pts)\n",
    "        pts = np.array([pts])\n",
    "        \n",
    "        try:\n",
    "            dst = cv2.perspectiveTransform(pts,H)\n",
    "            frame = cv2.polylines(frame,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        except:\n",
    "            print(\"error perspectiveTransform\")\n",
    "        \n",
    "        # Draw top matches\n",
    "        img_matches = cv2.drawMatches(frame, keypoints1, template, keypoints2, matches, None)\n",
    "\n",
    "        cv2.imshow('video', img_matches)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
